# LLM Evaluation tool

To Run evaluation: 

* Python 3.6+
* Required Python packages: `langchain`, `pandas`, `mlflow`, `logging`, `argparse`

Install the necessary Python packages by running

    pip install langchain pandas mlflow logging argparse


## Usage

The tool can be used via command line with various options for specifying criteria, input datasets, and output paths.

### Command Line Arguments

* `--criteria`: Specifies the list of grading criteria. Default: `correctness coherence contextuality informativeness fluency`
* `--output`: Output path to store evaluation data. Default: dynamically created based on current date and time.
* `--question`: Path for evaluation question dataset. Default: `eval_questions.csv`
* `--evaluation`: Path for LLM results to be evaluated. Default: `llm_answers.csv`

### Running the Evaluation

To run the evaluation with default settings:

`-python evaluator.py`

To specify custom criteria and paths:

`-python evaluator.py --criteria correctness coherence --question path/to/your/questions.csv --evaluation path/to/your/answers.csv --output path/to/output`



## Note on each file

1. **`evaluator.py`** :

* **Purpose** : This script is the core of the evaluation process. It reads questions from `eval_question.csv` and the corresponding LLM-generated answers from `result.csv`. In future iterations, it may also read a template for grading the answers to standardize and possibly automate the evaluation criteria.
* **Functionality** : Parsing the CSV files, applying grading logic to compare the LLM responses against the correct answers or evaluation criteria, and then summarizing the performance of each LLM.

2. **`generate_eval_question.py`** (In Progress):

* **Purpose** : Responsible for generating evaluative questions based on a given context. This script aims to produce a diverse and relevant set of questions that are used to test the LLMs.
* **Output** : The generated questions, along with their correct answers, are exported directly to `eval_question.csv`, ensuring that the evaluation process has a consistent and up-to-date set of questions.

3. **`run_llm.py`** (In Progress):

* **Purpose** : This script orchestrates the interaction with one or more LLMs to gather their responses to the questions listed in `eval_question.csv`.
* **Functionality** : It reads the questions from `eval_question.csv`, prompts the LLMs to generate answers, and then saves those answers directly into `result.csv` for evaluation.

4. **`eval_question.csv`** :

* **Contents** : Contains the set of questions and the correct answers that form the basis of the evaluation. This CSV file is essential for both generating LLM responses and for the evaluation script to know what the correct answers are.

5. **`result.csv`** :

* **Contents** : Holds the answers generated by the evaluated LLMs. Each LLM's responses to the questions in `eval_question.csv` are stored here, ready to be assessed by the evaluation script.

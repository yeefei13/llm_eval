
# LLM Eval

to run evaluation on models, run:

python llm_QA_eval.py 


- llm_QA_eval.py: runs the evaluation on llm result, only reading eval_question.csv and result.csv(potentially reading template for grading the answers in the future).

- generate_eval_question.py: llm generating questions according to given context, output will be directly export to eval_question.csv.

- run_llm.py: llms to be evaluated will generate their answer to the given questions in eval_question.csv here, the output will be directly export to result.csv.

- eval_question.csv: contains questions and correct answer that the evaluation will be based on

- result.csv: result generated by the llms

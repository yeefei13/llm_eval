# LLM Eval

to run evaluation on models, run:

python llm_QA_eval.py

1. **`llm_QA_eval.py`** :

* **Purpose** : This script is the core of the evaluation process. It reads questions from `eval_question.csv` and the corresponding LLM-generated answers from `result.csv`. In future iterations, it may also read a template for grading the answers to standardize and possibly automate the evaluation criteria.
* **Functionality** : Parsing the CSV files, applying grading logic to compare the LLM responses against the correct answers or evaluation criteria, and then summarizing the performance of each LLM.

2. **`generate_eval_question.py`** (In Progress):

* **Purpose** : Responsible for generating evaluative questions based on a given context. This script aims to produce a diverse and relevant set of questions that are used to test the LLMs.
* **Output** : The generated questions, along with their correct answers, are exported directly to `eval_question.csv`, ensuring that the evaluation process has a consistent and up-to-date set of questions.

3. **`run_llm.py`** (In Progress):

* **Purpose** : This script orchestrates the interaction with one or more LLMs to gather their responses to the questions listed in `eval_question.csv`.
* **Functionality** : It reads the questions from `eval_question.csv`, prompts the LLMs to generate answers, and then saves those answers directly into `result.csv` for evaluation.

4. **`eval_question.csv`** :

* **Contents** : Contains the set of questions and the correct answers that form the basis of the evaluation. This CSV file is essential for both generating LLM responses and for the evaluation script to know what the correct answers are.

5. **`result.csv`** :

* **Contents** : Holds the answers generated by the evaluated LLMs. Each LLM's responses to the questions in `eval_question.csv` are stored here, ready to be assessed by the evaluation script.